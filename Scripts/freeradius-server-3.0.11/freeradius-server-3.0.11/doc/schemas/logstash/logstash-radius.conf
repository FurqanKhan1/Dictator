# logstash configuration to process RADIUS detail files
#
# Matthew Newton
# January 2016
# 
# RADIUS "detail" files are textual representations of the RADIUS
# packets, and are written to disk by e.g. FreeRADIUS. They look
# something like the following, with the timestamp on the first
# line then all attributes/values tab-indented.
#
#	Tue Mar 10 15:32:24 2015
#		Packet-Type = Access-Request
#		User-Name = "test@example.com"
#		Calling-Station-Id = "01-02-03-04-05-06"
#		Called-Station-Id = "aa-bb-cc-dd-ee-ff:myssid"
#		NAS-Port = 10
#		NAS-IP-Address = 10.9.0.4
#		NAS-Identifier = "Wireless-Controller-1"
#		Service-Type = Framed-User
#		NAS-Port-Type = Wireless-802.11
#



# Example input - read data from a file. This can be useful for
# testing, but usually not so much for live service. For example,
# to read in a detail file with this input you could use:
#
#   /opt/logstash/bin/logstash -v -f logstash-radius.conf < detailfile

input {
	stdin {
		type => radiusdetail
	}
}

# Moving into production will likely need something more reliable.
# There are many input methods, an example here using log-courier
# (which supports client-site multiline processing and does not
# lose log events if logstash is restarted).

# input {
# 	courier {
# 		port => 5140
# 		transport => "tcp"
# 	}
# }



# Filter stage. Here we take the raw logs and process them into
# something structured ready to index. Each attribute is stored as
# a separate field in the output document.

filter {

	if [type] == "radiusdetail" {

		# If you are using a log feeder that can join
		# multiple lines together then that is preferrable
		# to using multiline here, because this can not be
		# used with threaded logstash (i.e. -w<n> at
		# startup).

		# In that case you should comment out the following
		# section. For example, see the log-courier
		# configuration configuration in this directory.

		multiline {
			pattern => "^[A-Z\t]"
			negate => false
			what => "next"
		}

		# Pull off the timestamp at the start of the
		# detail record. Note there may be additional data
		# after it that has been added by the local admin,
		# so stop at a newline OR a tab.

		grok {
			match => [ "message", "^(?<timestamp>[^\n\t]+)[\n\t]" ]
		}

		# Create the @timestamp field.

		date {
			match => [ "timestamp", "EEE MMM dd HH:mm:ss yyyy",
						"EEE MMM  d HH:mm:ss yyyy" ]
		}

		# Split the attributes and values into fields.
		# This is the bulk of processing that adds all of
		# the RADIUS attributes as elasticsearch fields.

		kv {
			field_split => "\n"
			source => "message"
			trim => "\" "
			trimkey => "\t "
		}

		# Now we try and add some useful additional
		# information. If certain fields can be broken
		# down into components then do that here and add
		# the data as sub-fields. For example,
		# Called-Station-Id might be able to be broken
		# down to Called-Station-Id_mac and Called-Station-Id_ssid
		# on some wireless systems, or to _ip and _port
		# with a VPN.

		# Multiple calls to grok otherwise it can stop
		# processing once it has matched one field, but
		# e.g. you want to pull both IP and port out of
		# the same field in two different regex's.

		# Pull out some IP addresses as field_ip:

		grok {
			break_on_match => false
			tag_on_failure => []
			match => [
				"Framed-IP-Address", "^(?<Framed-IP-Address_ip>\d+\.\d+\.\d+\.\d+$)",
				"NAS-IP-Address", "^(?<NAS-IP-Address_ip>\d+\.\d+\.\d+\.\d+$)",
				"Calling-Station-Id", "^(?<Calling-Station-Id_ip>\d+\.\d+\.\d+\.\d+)",
				"Called-Station-Id", "^(?<Called-Station-Id_ip>\d+\.\d+\.\d+\.\d+)"
			]
		}

		# Split User-Name, Operator-Name, and pull out
		# some IP ports if they are there:

		grok {
			break_on_match => false
			tag_on_failure => []
			match => [
				"User-Name", "^(?<User-Name_username>[^@]+)?(?:@(?<User-Name_realm>[^@]+))$",
				"Operator-Name", "^(?<Operator-Name_id>.)(?<Operator-Name_value>.+)$",

				"Calling-Station-Id", "\[(?<Calling-Station-Id_port>\d+)\]$",
				"Called-Station-Id", "\[(?<Called-Station-Id_port>\d+)\]$"
			]
		}

		# Extract MAC addresses (and SSIDs if there).
		# MAC address matching here is lazy, but should be
		# good enough.

		grok {
			break_on_match => false
			tag_on_failure => []
			match => [
				"Calling-Station-Id", "^(?<Calling-Station-Id_mac>[a-fA-F0-9:-]{17})$",
				"Calling-Station-Id", "^(?<Calling-Station-Id_mac>[a-fA-F0-9\.]{14})$",
				"Calling-Station-Id", "^(?<Calling-Station-Id_mac>[a-fA-F0-9]{12})$",

				"Called-Station-Id", "^(?<Called-Station-Id_mac>[a-fA-F0-9:-]{17})(?::(?<Called-Station-Id_ssid>.*))?$",
				"Called-Station-Id", "^(?<Called-Station-Id_mac>[a-fA-F0-9\.]{14})(?::(?<Called-Station-Id_ssid>.*))?$",
				"Called-Station-Id", "^(?<Called-Station-Id_mac>[a-fA-F0-9]{12})(?::(?<Called-Station-Id_ssid>.*))?$"
			]
		}

		# With the optional sanitize_mac plugin, it's
		# possible to make sure all MAC addresses look the
		# same, which has obvious benefits.
		#
		# https://github.com/mcnewton/elk/blob/master/logstash-filters/sanitize_mac.rb

		# sanitize_mac {
		# 	match => {
		# 		"Called-Station-Id_mac" => "Called-Station-Id_mac"
		# 		"Calling-Station-Id_mac" => "Calling-Station-Id_mac"
		# 		}
		# 	separator => ":"
		# 	fixcase => "lower"
		# }


		# Gigawords presents an issue because the 64-bit
		# value is split across two attributes. Combine
		# them both back into a single attribute so that
		# the full value is available to use.

		if ([Acct-Input-Octets]) {
			ruby {
				code => "event['Acct-Input-Octets_long'] =
					event['Acct-Input-Octets'].to_i + ( event['Acct-Input-Gigawords'] ? (event['Acct-Input-Gigawords'].to_i * (2**32)) : 0)"
			}
		}

		if ([Acct-Output-Octets]) {
			ruby {
				code => "event['Acct-Output-Octets_long'] =
					event['Acct-Output-Octets'].to_i + ( event['Acct-Output-Gigawords'] ? (event['Acct-Output-Gigawords'].to_i * (2**32)) : 0)"
			}
		}

	}
}



# Output data to the local elasticsearch cluster (called
# "elasticsearch") using type "detail" in index "radius-DATE".

output {
	if [type] == "radiusdetail" {
		elasticsearch {
			host => localhost
			protocol => http
			cluster => elasticsearch
			index_type => "detail"
			index => "radius-%{+YYYY.MM.dd}"
			flush_size => 1000
		}
	}
}

